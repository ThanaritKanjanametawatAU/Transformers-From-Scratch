# Transformers From Scratch

This project implements the Transformer architecture from scratch, based on the groundbreaking paper "Attention Is All You Need".

## Overview

In this project, we build the Transformer model step-by-step, implementing all the key components including:
- Multi-Head Attention mechanism
- Positional Encoding
- Encoder and Decoder stacks
- Feed-Forward Networks
- Layer Normalization

## References

- **Original Paper**: [Attention Is All You Need](https://arxiv.org/abs/1706.03762) (Vaswani et al., 2017)
- **Tutorial Video**: [Building Transformers from Scratch](https://youtu.be/ISNdQcPhsts?si=-gqCE1E36xpbVcjE)

## Project Structure

```
Transformers-From-Scratch/
├── notebooks/          # Jupyter notebooks for experiments
├── src/               # Source code implementation
└── README.md          # This file
```

## Getting Started

Follow along with the tutorial video and implement each component step by step to understand how Transformers work under the hood.